{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "SEED = 42\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, log_loss, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data's first two rows are headers.\n",
    "The first row contains the configuration numbers, but spaced one apart.\n",
    "The second row contains the completion time and mistakes under each configuration number column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df. We can make each row contain the config num, completion time, and mistakes.\n",
    "# Therefore, the data will contain 3 columns: config_num, completion_time, mistakes\n",
    "# and the data will multiply the number of rows by the 7 (because there are 7 configs)\n",
    "\n",
    "converted_df:pd.DataFrame = pd.DataFrame(columns=['nick', 'config_num', 'completion_time', 'mistakes'])\n",
    "\n",
    "# For each participant\n",
    "num_participants = df.shape[0] - 1 # -1 because the first two rows are headers\n",
    "\n",
    "for i in range(num_participants):\n",
    "    # For each config\n",
    "    for j in range(1, 8):\n",
    "        config_num = j-1\n",
    "        \n",
    "        # Mistakes and completion time are in the same row\n",
    "        # But they are in different columns. Each config has 2 columns for mistakes and completion time\n",
    "        mistakes_col = j * 2\n",
    "        completion_time_col = j * 2 - 1\n",
    "\n",
    "        mistakes = df.iloc[i+1, mistakes_col]\n",
    "        completion_time = df.iloc[i+1, completion_time_col]\n",
    "\n",
    "        # Add row to converted_df\n",
    "        converted_df = pd.concat([converted_df, pd.DataFrame([[df.iloc[i+1, 0], config_num, completion_time, mistakes]], columns=['nick', 'config_num', 'completion_time', 'mistakes'])])\n",
    "\n",
    "        # Remove index\n",
    "        converted_df = converted_df.reset_index(drop=True)\n",
    "\n",
    "converted_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Previous shape: \", df.shape)\n",
    "print(\"New shape: \", converted_df.shape)\n",
    "\n",
    "df = converted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our current data, the data uses configurations. However, that does not tell us enough. We can add three new variables (`size`, `color`,  and `position`) to give us more insights about our data.\n",
    "- The baseline by default has `size: regular`, `color: yellow`, and `position: top`.\n",
    "- Config 1 and 2 changes the size into `small` and `large`\n",
    "- Config 3 and 4 changes the color into `blue` and `black`\n",
    "- Config 5 and 6 changes the position into `sticky` and `bottom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column for size, color, and position. Filled based on config_num\n",
    "\n",
    "df['size'] = df['config_num'].apply(lambda x: 'regular' if x == 0 or x == 3 or x == 4 else 'small' if x == 1 else 'large')\n",
    "df['color'] = df['config_num'].apply(lambda x: 'yellow' if x == 0 or x == 1 or x == 2 or x == 5 else 'blue' if x == 3 else 'black' if x == 4 else 'yellow')\n",
    "df['position'] = df['config_num'].apply(lambda x: 'top' if x == 0 or x == 1 or x == 2 or x == 3 or x == 4 else 'sticky' if x == 5 else 'bottom')\n",
    "\n",
    "df.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data currently has outliers so we have to remove them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "\n",
    "# Convert size, color, and position to numerical values (0, 1, 2)\n",
    "# df_cleaned['size'] = df_cleaned['size'].apply(lambda x: 1 if x == 'small' else 0 if x == 'regular' else 2)\n",
    "# df_cleaned['color'] = df_cleaned['color'].apply(lambda x: 1 if x == 'blue' else 2 if x == 'black' else 0)\n",
    "# df_cleaned['position'] = df_cleaned['position'].apply(lambda x: 1 if x == 'sticky' else 2 if x == 'top' else 0)\n",
    "\n",
    "# Remove outliers based on completion time (and possibly other features, just edit the features_of_interest)\n",
    "features_of_interest = ['completion_time']\n",
    "clf = IsolationForest(max_samples=100, random_state=SEED)\n",
    "clf.fit(df_cleaned[features_of_interest])\n",
    "df_cleaned['anomaly'] = clf.predict(df_cleaned[features_of_interest])\n",
    "\n",
    "# Plot the data to reveal outliers in completion time using histograms\n",
    "# Make anomalies red x's and normal data blue dots\n",
    "completion_times = np.random.normal(loc=50, scale=10, size=1000)\n",
    "\n",
    "# Create a histogram with adjusted x-axis labels and outliers in red\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram for normal data in blue\n",
    "plt.hist(completion_times, bins=20, alpha=0.5, color='blue', edgecolor='black', label='Normal')\n",
    "\n",
    "# Detect outliers using IQR or other methods (replace this with your outlier detection code)\n",
    "Q1 = np.percentile(completion_times, 25)\n",
    "Q3 = np.percentile(completion_times, 75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_threshold = 1.5 * IQR\n",
    "outliers = completion_times[(completion_times < Q1 - outlier_threshold) | (completion_times > Q3 + outlier_threshold)]\n",
    "\n",
    "# Plot histogram bars for outliers in red\n",
    "plt.hist(outliers, bins=20, alpha=0.5, color='red', edgecolor='black', label='Outliers')\n",
    "\n",
    "plt.xlabel('Completion Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Completion Time')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.legend()\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n",
    "\n",
    "# Filter out outliers based on completion time\n",
    "df_cleaned = df_cleaned[df_cleaned['anomaly'] == 1]\n",
    "\n",
    "# Drop the 'anomaly' column\n",
    "df_cleaned = df_cleaned.drop('anomaly', axis=1)\n",
    "\n",
    "print(\"Previous shape:\", df.shape)\n",
    "print(\"New shape after removing outliers based on completion time:\", df_cleaned.shape)\n",
    "\n",
    "df_cleaned.head(14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert our initial categorical features into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert size, color, and position to numerical values (0, 1, 2)\n",
    "df_cleaned['size'] = df_cleaned['size'].apply(lambda x: 1 if x == 'small' else 0 if x == 'regular' else 2)\n",
    "df_cleaned['color'] = df_cleaned['color'].apply(lambda x: 1 if x == 'blue' else 2 if x == 'black' else 0)\n",
    "df_cleaned['position'] = df_cleaned['position'].apply(lambda x: 1 if x == 'sticky' else 2 if x == 'top' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us decide whether we would keep the mistakes column as a feature of the our model, we first check how many mistakes people made in our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_num = df_cleaned['mistakes'].value_counts()\n",
    "mistakes_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_rate = mistakes_num[1] / (mistakes_num[0] + mistakes_num[1])\n",
    "mistake_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mistake rate is low, we can leave it out. The features of the model are the size, color, and position, and the predicted value is the completion time. The config_num is also disregarded, as it is redundant. It can be derived from the unique combination of size, color, and position. This avoids multicollinearity in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin splitting the data for our models to use. We conduct an 80-20 train-test split. Validation split is no longer needed because it's done by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, and test sets. Goal is completion time\n",
    "X = df_cleaned[['size', 'color', 'position']]\n",
    "y = df_cleaned['completion_time']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Count number of samples per split\n",
    "print(\"Total number of samples:\", len(X))\n",
    "print(\"Number of samples in training set:\", len(X_train))\n",
    "print(\"Number of samples in test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call RandomForestRegressor from scikitlearn and fit our training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = rf.predict(X_train)\n",
    "test_predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the predictions, we evaluate our model using the mean absolute error (MAE), mean squared error (MSE), R^2, and mean absolute percentage error (MAPE) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'y_test' and 'y_test_pred' to numeric types\n",
    "y_test = pd.to_numeric(y_test)\n",
    "test_predictions = pd.to_numeric(test_predictions)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "mse_test = mean_squared_error(y_test, test_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "mape = np.mean(np.abs((y_test - test_predictions) / np.abs(y_test))) \n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "print(f'Test MAE: {mae_test:.4f}')\n",
    "print(f'Test MSE: {mse_test:.4f}')\n",
    "print(f'Test R^2 Score: {r2_test:.4f}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape * 100:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context to the MAE metric, we get the range of the completion times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['completion_time'] = df_cleaned['completion_time'].astype(float)\n",
    "max_time = df_cleaned['completion_time'].max()\n",
    "min_time = df_cleaned['completion_time'].min()\n",
    "min_time, max_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relatively low MAE is indicates that on average, the model's predictions are pretty close to the actual values. This is a relatively low value as the range of the predicted value (completion time) is from 21.826 to 50.157.\n",
    "\n",
    "The MSE metric is fairly high. This may be due to the limited number of samples in the dataset, where the model is not able to identify key patterns in the data to be able to make predictions effectively.\n",
    "\n",
    "The low R^2 score suggests that the model explains a small portion of variability in the target variable.\n",
    "\n",
    "The MAPE value indicates that on average, the predictions are 15.71% off from the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use GridSearchCV to find the best parameters for the model that would lead to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Searching (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above parameters turned out to be optimal for our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a new model with our optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf = RandomForestRegressor(**best_params)\n",
    "tuned_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = tuned_rf.predict(X_train)\n",
    "test_predictions = tuned_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'y_test' and 'y_test_pred' to numeric types\n",
    "y_test = pd.to_numeric(y_test)\n",
    "test_predictions = pd.to_numeric(test_predictions)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "mse_test = mean_squared_error(y_test, test_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "mape = np.mean(np.abs((y_test - test_predictions) / np.abs(y_test))) \n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "print(f'Test MAE: {mae_test:.4f}')\n",
    "print(f'Test MSE: {mse_test:.4f}')\n",
    "print(f'Test R^2 Score: {r2_test:.4f}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape * 100:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics of the tuned model is very similar to the performance of the original model. The MAE and MSE are slightly higher in the tuned model than in the original model, which indicates that the tuned model are slightly further from the actual values.\n",
    "\n",
    "The R^2 score is slightly lower for the tuned model, which suggests that its explains less variance in the target variable compared to the original model.\n",
    "\n",
    "The MAPE is slightly lower for the tuned model, indicating that the predicted completion times are slightly closer to the actual values.\n",
    "\n",
    "Overall, the tuning process did not significantly improve the model's performance. One possible reason is that the model was already performing at its maximum performance even before the tuning. The nature of the dataset such as the features and the number of samples is also a possible reason for the insignficant difference in model performance before and after the tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
