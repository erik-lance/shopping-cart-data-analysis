{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "SEED = 42\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, log_loss, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data's first two rows are headers.\n",
    "The first row contains the configuration numbers, but spaced one apart.\n",
    "The second row contains the completion time and mistakes under each configuration number column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df. We can make each row contain the config num, completion time, and mistakes.\n",
    "# Therefore, the data will contain 3 columns: config_num, completion_time, mistakes\n",
    "# and the data will multiply the number of rows by the 7 (because there are 7 configs)\n",
    "\n",
    "converted_df:pd.DataFrame = pd.DataFrame(columns=['nick', 'config_num', 'completion_time', 'mistakes'])\n",
    "\n",
    "# For each participant\n",
    "num_participants = df.shape[0] - 1 # -1 because the first two rows are headers\n",
    "\n",
    "for i in range(num_participants):\n",
    "    # For each config\n",
    "    for j in range(1, 8):\n",
    "        config_num = j-1\n",
    "\n",
    "        # Mistakes and completion time are in the same row\n",
    "        # But they are in different columns. Each config has 2 columns for mistakes and completion time\n",
    "        mistakes_col = j * 2\n",
    "        completion_time_col = j * 2 - 1\n",
    "\n",
    "        mistakes = df.iloc[i+1, mistakes_col]\n",
    "        completion_time = df.iloc[i+1, completion_time_col]\n",
    "\n",
    "        # Add row to converted_df\n",
    "        converted_df = pd.concat([converted_df, pd.DataFrame([[df.iloc[i+1, 0], config_num, completion_time, mistakes]], columns=['nick', 'config_num', 'completion_time', 'mistakes'])])\n",
    "\n",
    "        # Remove index\n",
    "        converted_df = converted_df.reset_index(drop=True)\n",
    "\n",
    "converted_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Previous shape: \", df.shape)\n",
    "print(\"New shape: \", converted_df.shape)\n",
    "\n",
    "df = converted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our current data, the data uses configurations. However, that does not tell us enough. We can add three new variables (`size`, `color`,  and `position`) to give us more insights about our data.\n",
    "- The baseline by default has `size: regular`, `color: yellow`, and `position: top`.\n",
    "- Config 1 and 2 changes the size into `small` and `large`\n",
    "- Config 3 and 4 changes the color into `blue` and `black`\n",
    "- Config 5 and 6 changes the position into `sticky` and `bottom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column for size, color, and position. Filled based on config_num\n",
    "\n",
    "df['size'] = df['config_num'].apply(lambda x: 'regular' if x == 0 or x == 3 or x == 4 else 'small' if x == 1 else 'large')\n",
    "df['color'] = df['config_num'].apply(lambda x: 'yellow' if x == 0 or x == 1 or x == 2 or x == 5 else 'blue' if x == 3 else 'black' if x == 4 else 'yellow')\n",
    "df['position'] = df['config_num'].apply(lambda x: 'top' if x == 0 or x == 1 or x == 2 or x == 3 or x == 4 else 'sticky' if x == 5 else 'bottom')\n",
    "\n",
    "df.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data currently has outliers so we have to remove them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "\n",
    "# Convert size, color, and position to numerical values (0, 1, 2)\n",
    "# df_cleaned['size'] = df_cleaned['size'].apply(lambda x: 1 if x == 'small' else 0 if x == 'regular' else 2)\n",
    "# df_cleaned['color'] = df_cleaned['color'].apply(lambda x: 1 if x == 'blue' else 2 if x == 'black' else 0)\n",
    "# df_cleaned['position'] = df_cleaned['position'].apply(lambda x: 1 if x == 'sticky' else 2 if x == 'top' else 0)\n",
    "\n",
    "# Remove outliers based on completion time (and possibly other features, just edit the features_of_interest)\n",
    "features_of_interest = ['completion_time']\n",
    "clf = IsolationForest(max_samples=100, random_state=SEED)\n",
    "clf.fit(df_cleaned[features_of_interest])\n",
    "df_cleaned['anomaly'] = clf.predict(df_cleaned[features_of_interest])\n",
    "\n",
    "# Plot the data to reveal outliers in completion time using histograms\n",
    "# Make anomalies red x's and normal data blue dots\n",
    "completion_times = np.random.normal(loc=50, scale=10, size=1000)\n",
    "\n",
    "# Create a histogram with adjusted x-axis labels and outliers in red\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram for normal data in blue\n",
    "plt.hist(completion_times, bins=20, alpha=0.5, color='blue', edgecolor='black', label='Normal')\n",
    "\n",
    "# Detect outliers using IQR or other methods (replace this with your outlier detection code)\n",
    "Q1 = np.percentile(completion_times, 25)\n",
    "Q3 = np.percentile(completion_times, 75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_threshold = 1.5 * IQR\n",
    "outliers = completion_times[(completion_times < Q1 - outlier_threshold) | (completion_times > Q3 + outlier_threshold)]\n",
    "\n",
    "# Plot histogram bars for outliers in red\n",
    "plt.hist(outliers, bins=20, alpha=0.5, color='red', edgecolor='black', label='Outliers')\n",
    "\n",
    "plt.xlabel('Completion Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Completion Time')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.legend()\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n",
    "\n",
    "# Filter out outliers based on completion time\n",
    "df_cleaned = df_cleaned[df_cleaned['anomaly'] == 1]\n",
    "\n",
    "# Drop the 'anomaly' column\n",
    "df_cleaned = df_cleaned.drop('anomaly', axis=1)\n",
    "\n",
    "print(\"Previous shape:\", df.shape)\n",
    "print(\"New shape after removing outliers based on completion time:\", df_cleaned.shape)\n",
    "\n",
    "df_cleaned.head(14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Breakdown\n",
    "\n",
    "Since we've cleaned and preprocessed the data, let's now see the completion times without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the mean, standard deviation, and quartiles of the completion time of each config (remove feaatures back)\n",
    "df_analysis = df_cleaned.drop(['nick', 'size', 'color', 'position'], axis=1)\n",
    "df_analysis = df_analysis.groupby('config_num')\n",
    "\n",
    "# Print best and worst configurations based on completion time\n",
    "print(\"Best configuration based on completion time:\")\n",
    "print(df_analysis['completion_time'].mean().idxmin())\n",
    "print(\"Worst configuration based on completion time:\")\n",
    "print(df_analysis['completion_time'].mean().idxmax())\n",
    "\n",
    "df_analysis.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert our initial categorical features into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert size, color, and position to numerical values (0, 1, 2)\n",
    "df_cleaned['size'] = df_cleaned['size'].apply(lambda x: 1 if x == 'small' else 0 if x == 'regular' else 2)\n",
    "df_cleaned['color'] = df_cleaned['color'].apply(lambda x: 1 if x == 'blue' else 2 if x == 'black' else 0)\n",
    "df_cleaned['position'] = df_cleaned['position'].apply(lambda x: 1 if x == 'sticky' else 2 if x == 'top' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us decide whether we would keep the mistakes column as a feature of the our model, we first check how many mistakes people made in our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_num = df_cleaned['mistakes'].value_counts()\n",
    "mistakes_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_rate = mistakes_num[1] / (mistakes_num[0] + mistakes_num[1])\n",
    "mistake_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mistake rate is low, we can leave it out. The features of the model are the size, color, and position, and the predicted value is the completion time. The config_num is also disregarded, as it is redundant. It can be derived from the unique combination of size, color, and position. This avoids multicollinearity in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin splitting the data for our models to use. We conduct an 80-20 train-test split. Validation split is no longer needed because it's done by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, and test sets. Goal is completion time\n",
    "X = df_cleaned[['size', 'color', 'position']]\n",
    "y = df_cleaned['completion_time']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Count number of samples per split\n",
    "print(\"Total number of samples:\", len(X))\n",
    "print(\"Number of samples in training set:\", len(X_train))\n",
    "print(\"Number of samples in test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call RandomForestRegressor from scikitlearn and fit our training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = rf.predict(X_train)\n",
    "test_predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the predictions, we evaluate our model using the mean absolute error (MAE), mean squared error (MSE), R^2, and mean absolute percentage error (MAPE) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'y_test' and 'y_test_pred' to numeric types\n",
    "y_test = pd.to_numeric(y_test)\n",
    "test_predictions = pd.to_numeric(test_predictions)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "mse_test = mean_squared_error(y_test, test_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "mape = np.mean(np.abs((y_test - test_predictions) / np.abs(y_test)))\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "training_evaluation_df = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R^2', 'Mean Absolute Percentage Error'],\n",
    "    'Value': [mae_test, mse_test, r2_test, mape]\n",
    "})\n",
    "\n",
    "print(\"Training Evaluation Metrics:\")\n",
    "print(training_evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context to the MAE metric, we get the range of the completion times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['completion_time'] = df_cleaned['completion_time'].astype(float)\n",
    "max_time = df_cleaned['completion_time'].max()\n",
    "min_time = df_cleaned['completion_time'].min()\n",
    "min_time, max_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relatively low MAE is indicates that on average, the model's predictions are pretty close to the actual values. This is a relatively low value as the range of the predicted value (completion time) is from `21.826` to `50.157`.\n",
    "\n",
    "The MSE metric is fairly high. This may be due to the limited number of samples in the dataset, where the model is not able to identify key patterns in the data to be able to make predictions effectively.\n",
    "\n",
    "The low R^2 score suggests that the model explains a small portion of variability in the target variable.\n",
    "\n",
    "The MAPE value indicates that on average, the predictions are `15.90%` off from the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use GridSearchCV to find the best parameters for the model that would lead to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Searching (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above parameters turned out to be optimal for our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a new model with our optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf = RandomForestRegressor(**best_params)\n",
    "tuned_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = tuned_rf.predict(X_train)\n",
    "test_predictions = tuned_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'y_test' and 'y_test_pred' to numeric types\n",
    "y_test = pd.to_numeric(y_test)\n",
    "test_predictions = pd.to_numeric(test_predictions)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "mse_test = mean_squared_error(y_test, test_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "mape = np.mean(np.abs((y_test - test_predictions) / np.abs(y_test)))\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "tuned_evaluation_df = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R^2', 'Mean Absolute Percentage Error'],\n",
    "    'Value': [mae_test, mse_test, r2_test, mape]\n",
    "})\n",
    "\n",
    "print(\"Tuned Evaluation Metrics:\")\n",
    "print(tuned_evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the evaluation metrics of the baseline and tuned models\n",
    "# Create a DataFrame to store the evaluation metrics by comining the baseline and tuned evaluation metrics.\n",
    "# Combine them on the 'Metric' column\n",
    "evaluation_metrics_df = pd.merge(training_evaluation_df, tuned_evaluation_df, on='Metric', suffixes=('_baseline', '_tuned'))\n",
    "\n",
    "# Add a column to show the improvement in the metric after tuning\n",
    "evaluation_metrics_df['Improvement'] = evaluation_metrics_df['Value_baseline'] - evaluation_metrics_df['Value_tuned']\n",
    "\n",
    "evaluation_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the evaluation metrics of the baseline model and the hyperparameter tuned model, we observe:\n",
    "- `01.43%` improvement in *Mean Absolute Error (MAE)*\n",
    "- `16.26%` improvement in *Mean Squared Error (MSE)*\n",
    "- `-00.35%` improvement in *R^2*; and\n",
    "- `00.07%` improvement in *Mean Absolute Percentage Error (MAPE)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics of the tuned model is very similar to the performance of the original model. The MAE and MSE are slightly lesser in the tuned model than in the original model, which indicates that the tuned model are slightly closer to the actual values.\n",
    "\n",
    "The R^2 score is slightly higher for the tuned model, which suggests that its explains more variance in the target variable compared to the original model.\n",
    "\n",
    "The MAPE is slightly lower for the tuned model, indicating that the predicted completion times are slightly closer to the actual values.\n",
    "\n",
    "Overall, despite the slight improvements, the tuning process did not significantly improve the model's performance. One possible reason is that the model was already performing at its maximum performance even before the tuning. The nature of the dataset such as the features and the number of samples is also a possible reason for the insignficant difference in model performance before and after the tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model trained, we can also figure out how important each feature is in predicting the completion time. Let's see what these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the tuned random forest model\n",
    "importances_df = pd.DataFrame(tuned_rf.feature_importances_, index=['size', 'color', 'position'], columns=['Importance'])\n",
    "importances_df = importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Resetting the index to convert the dataframe into a long-format dataframe expected by seaborn\n",
    "importances_df.reset_index(inplace=True)\n",
    "importances_df.rename(columns={'index': 'Feature'}, inplace=True)\n",
    "\n",
    "# Horizontal bar plot of feature importances\n",
    "plt.figure(figsize=(8, 2))\n",
    "sns.barplot(x='Importance', y=\"Feature\", data=importances_df, hue=importances_df.index, dodge=False)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print values\n",
    "importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that `color` plays the largest role in affecting a participant's completion time, where it contributes `37.03%` in the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also figure out which features will provide us the lowest completion time. Let's grab the sample with the lowest completion time and see what these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lowest possible completion time by going through all possible combinations of size, color, and position\n",
    "# Create a list of all possible combinations of size, color, and position\n",
    "size_values = [0, 1, 2]\n",
    "color_values = [0, 1, 2]\n",
    "position_values = [0, 1, 2]\n",
    "\n",
    "# Combine all possible values of size, color, and position\n",
    "all_combinations = [(size, color, position) for size in size_values for color in color_values for position in position_values]\n",
    "\n",
    "# Predict completion times for all possible combinations\n",
    "completion_times = tuned_rf.predict(all_combinations)\n",
    "\n",
    "# Find the combination with the lowest completion time\n",
    "min_completion_time = min(completion_times)\n",
    "min_completion_time_idx = np.argmin(completion_times)\n",
    "best_size, best_color, best_position = all_combinations[min_completion_time_idx]\n",
    "\n",
    "# Create a dataframe to store the completion times for all possible combinations\n",
    "completion_times_df = pd.DataFrame({\n",
    "    'Size': [size for size, _, _ in all_combinations],\n",
    "    'Color': [color for _, color, _ in all_combinations],\n",
    "    'Position': [position for _, _, position in all_combinations],\n",
    "    'Completion Time': completion_times\n",
    "})\n",
    "\n",
    "# Find the highest possible completion time by going through all possible combinations of size, color, and position\n",
    "# Find the combination with the highest completion time\n",
    "max_completion_time = max(completion_times)\n",
    "max_completion_time_idx = np.argmax(completion_times)\n",
    "worst_size, worst_color, worst_position = all_combinations[max_completion_time_idx]\n",
    "\n",
    "# Add the best and worst combinations to the completion_times_df\n",
    "completion_times_df = pd.concat([completion_times_df, pd.DataFrame({\n",
    "    'Size': [best_size, worst_size],\n",
    "    'Color': [best_color, worst_color],\n",
    "    'Position': [best_position, worst_position],\n",
    "    'Completion Time': [min_completion_time, max_completion_time]\n",
    "})], ignore_index=True)\n",
    "\n",
    "\n",
    "# Print the best and worst completion times and their corresponding configurations through the df\n",
    "print(\"Best Configuration:\")\n",
    "print(completion_times_df[completion_times_df['Completion Time'] == min_completion_time])\n",
    "\n",
    "print(\"\\nWorst Configuration:\")\n",
    "print(completion_times_df[completion_times_df['Completion Time'] == max_completion_time])\n",
    "\n",
    "# Plot the completion times for all possible combinations\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot of completion times for all possible combinations\n",
    "sc = ax.scatter(completion_times_df['Size'], completion_times_df['Color'], completion_times_df['Position'], c=completion_times_df['Completion Time'], cmap='viridis', s=100)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Size')\n",
    "ax.set_ylabel('Color')\n",
    "ax.set_zlabel('Position')\n",
    "plt.title('Completion Times for All Possible Combinations')\n",
    "\n",
    "# Add a colorbar to show the completion time values\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Completion Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the best size, color, and position in a human-readable format\n",
    "size_dict = {0: 'Regular', 1: 'Small', 2: 'Large'}\n",
    "color_dict = {0: 'Yellow', 1: 'Blue', 2: 'Black'}\n",
    "position_dict = {0: 'Top', 1: 'Sticky', 2: 'Bottom'}\n",
    "\n",
    "best_size_str = size_dict[best_size]\n",
    "best_color_str = color_dict[best_color]\n",
    "best_position_str = position_dict[best_position]\n",
    "\n",
    "worst_size_str = size_dict[worst_size]\n",
    "worst_color_str = color_dict[worst_color]\n",
    "worst_position_str = position_dict[worst_position]\n",
    "\n",
    "configuration_df = pd.DataFrame({\n",
    "    'Configuration': ['Best', 'Worst'],\n",
    "    'Size': [best_size_str, worst_size_str],\n",
    "    'Color': [best_color_str, worst_color_str],\n",
    "    'Position': [best_position_str, worst_position_str],\n",
    "    'Completion Time': [min_completion_time, max_completion_time]\n",
    "})\n",
    "\n",
    "configuration_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon further inspection, we find out that the best configuration was our original `Configuration 1` which was the small button size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Attention Score and Completion Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the heatmap_analysis.ipynb file for details on how the attention scores were derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_results = pd.read_csv('heatmap_analysis_results.csv')\n",
    "heatmap_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we check the relationship between the attention score of the checkout button and the completion time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkout = heatmap_results['checkout']\n",
    "\n",
    "avg_completion_time = df_cleaned.groupby(['config_num'])['completion_time'].mean()\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'pink']\n",
    "\n",
    "for config_num, color in zip(avg_completion_time.index, colors):\n",
    "    plt.scatter(checkout[avg_completion_time.index == config_num],\n",
    "                avg_completion_time[avg_completion_time.index == config_num],\n",
    "                color=color,\n",
    "                label=f'Config Num: {config_num}')\n",
    "\n",
    "plt.xlabel('Checkout Attention Score')\n",
    "plt.ylabel('Average Completion Time')\n",
    "plt.title('Scatterplot of Checkout Attention Score vs Average Completion Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_checkout_corr = avg_completion_time.corr(checkout)\n",
    "ct_checkout_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative correlation indicates that as the completion time increases, the attention put on the checkout button decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we investigate the relationship between the attention score of the quantity dropdowns and the completion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_score = heatmap_results.iloc[:, :7].mean()\n",
    "\n",
    "avg_completion_time = df_cleaned.groupby(['config_num'])['completion_time'].mean()\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'pink']\n",
    "\n",
    "for config_num, color in zip(avg_completion_time.index, colors):\n",
    "    plt.scatter(qty_score[avg_completion_time.index == config_num],\n",
    "                avg_completion_time[avg_completion_time.index == config_num],\n",
    "                color=color,\n",
    "                label=f'Config Num: {config_num}')\n",
    "\n",
    "plt.xlabel('Qty Button Attention Score')\n",
    "plt.ylabel('Average Completion Time')\n",
    "plt.title('Scatterplot of Qty Button Attention Score vs Average Completion Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_score = qty_score.reset_index(drop=True)\n",
    "ct_qty_corr = avg_completion_time.corr(qty_score)\n",
    "ct_qty_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative correlation indicates that as the completion time increases, the attention put on the quantity dropdowns decreases. The quantitiy dropdowns have a stronger negative correlation with the completion times compared to the checkout button. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model dataset is quite simple (config number) and was simply pre-processed into several features based on the configuration number, let's see how it's like with different model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the different variations, we will mix the features by removing one feature at a time\n",
    "# cleaned_df variations\n",
    "cleaned_df_no_size = df_cleaned.drop('size', axis=1)\n",
    "cleaned_df_no_color = df_cleaned.drop('color', axis=1)\n",
    "cleaned_df_no_position = df_cleaned.drop('position', axis=1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_no_size = cleaned_df_no_size[['color', 'position']]\n",
    "X_no_color = cleaned_df_no_color[['size', 'position']]\n",
    "X_no_position = cleaned_df_no_position[['size', 'color']]\n",
    "y = df_cleaned['completion_time']\n",
    "\n",
    "X_train_no_size, X_test_no_size, y_train, y_test = train_test_split(X_no_size, y, test_size=0.2, random_state=SEED)\n",
    "X_train_no_color, X_test_no_color, y_train, y_test = train_test_split(X_no_color, y, test_size=0.2, random_state=SEED)\n",
    "X_train_no_position, X_test_no_position, y_train, y_test = train_test_split(X_no_position, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Train a random forest model on each variation\n",
    "rf_no_size = RandomForestRegressor(random_state=SEED)\n",
    "rf_no_color = RandomForestRegressor(random_state=SEED)\n",
    "rf_no_position = RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "rf_no_size.fit(X_train_no_size, y_train)\n",
    "rf_no_color.fit(X_train_no_color, y_train)\n",
    "rf_no_position.fit(X_train_no_position, y_train)\n",
    "\n",
    "# Make predictions on the test set for each variation\n",
    "test_predictions_no_size = rf_no_size.predict(X_test_no_size)\n",
    "test_predictions_no_color = rf_no_color.predict(X_test_no_color)\n",
    "test_predictions_no_position = rf_no_position.predict(X_test_no_position)\n",
    "\n",
    "# Calculate evaluation metrics for each variation\n",
    "mae_no_size = mean_absolute_error(y_test, test_predictions_no_size)\n",
    "mae_no_color = mean_absolute_error(y_test, test_predictions_no_color)\n",
    "mae_no_position = mean_absolute_error(y_test, test_predictions_no_position)\n",
    "\n",
    "mse_no_size = mean_squared_error(y_test, test_predictions_no_size)\n",
    "mse_no_color = mean_squared_error(y_test, test_predictions_no_color)\n",
    "mse_no_position = mean_squared_error(y_test, test_predictions_no_position)\n",
    "\n",
    "r2_no_size = r2_score(y_test, test_predictions_no_size)\n",
    "r2_no_color = r2_score(y_test, test_predictions_no_color)\n",
    "r2_no_position = r2_score(y_test, test_predictions_no_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation metrics for each variation\n",
    "evaluation_metrics_no_size = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R^2'],\n",
    "    'Value': [mae_no_size, mse_no_size, r2_no_size]\n",
    "})\n",
    "\n",
    "evaluation_metrics_no_color = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R^2'],\n",
    "    'Value': [mae_no_color, mse_no_color, r2_no_color]\n",
    "})\n",
    "\n",
    "evaluation_metrics_no_position = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R^2'],\n",
    "    'Value': [mae_no_position, mse_no_position, r2_no_position]\n",
    "})\n",
    "\n",
    "# Column names for the evaluation metrics DataFrames\n",
    "evaluation_metrics_no_size['Feature'] = 'Size'\n",
    "evaluation_metrics_no_color['Feature'] = 'Color'\n",
    "evaluation_metrics_no_position['Feature'] = 'Position'\n",
    "\n",
    "# Combine the evaluation metrics for each variation into a single DataFrame\n",
    "evaluation_metrics_combined = pd.concat([evaluation_metrics_no_size, evaluation_metrics_no_color, evaluation_metrics_no_position])\n",
    "\n",
    "evaluation_metrics_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the fastest completion time and its features usign the variations of cleaned_df\n",
    "# Find the lowest possible completion time by going through all possible combinations of size, color, and position\n",
    "# Create a list of all possible combinations of size, color, and position\n",
    "feature_one_values = [0, 1, 2]\n",
    "feature_two_values = [0, 1, 2]\n",
    "\n",
    "# Combine all possible values of size, color, and position\n",
    "all_combinations = [(feature_one, feature_two) for feature_one in feature_one_values for feature_two in feature_two_values]\n",
    "\n",
    "# Predict completion times for all possible combinations using the models trained on the variations\n",
    "completion_times_no_size = rf_no_size.predict(all_combinations)\n",
    "completion_times_no_color = rf_no_color.predict(all_combinations)\n",
    "completion_times_no_position = rf_no_position.predict(all_combinations)\n",
    "\n",
    "# Create a dataframe to store the completion times for all possible combinations\n",
    "completion_times_no_size_df = pd.DataFrame({\n",
    "    'Color': [feature_one for feature_one, _ in all_combinations],\n",
    "    'Position': [feature_two for _, feature_two in all_combinations],\n",
    "    'Completion Time': completion_times_no_size\n",
    "})\n",
    "\n",
    "completion_times_no_color_df = pd.DataFrame({\n",
    "    'Size': [feature_one for feature_one, _ in all_combinations],\n",
    "    'Position': [feature_two for _, feature_two in all_combinations],\n",
    "    'Completion Time': completion_times_no_color\n",
    "})\n",
    "\n",
    "completion_times_no_position_df = pd.DataFrame({\n",
    "    'Size': [feature_one for feature_one, _ in all_combinations],\n",
    "    'Color': [feature_two for _, feature_two in all_combinations],\n",
    "    'Completion Time': completion_times_no_position\n",
    "})\n",
    "\n",
    "# Add the missing feature as -1 to each variation\n",
    "completion_times_no_size_df['Size'] = -1\n",
    "completion_times_no_color_df['Color'] = -1\n",
    "completion_times_no_position_df['Position'] = -1\n",
    "\n",
    "# Find the lowest completion time for each variation\n",
    "min_completion_time_no_size = min(completion_times_no_size)\n",
    "min_completion_time_no_color = min(completion_times_no_color)\n",
    "min_completion_time_no_position = min(completion_times_no_position)\n",
    "\n",
    "# Get feature values for the best completion time for each variation\n",
    "best_no_size_features_idx = np.argmin(completion_times_no_size)\n",
    "best_no_color_features_idx = np.argmin(completion_times_no_color)\n",
    "best_no_position_features_idx = np.argmin(completion_times_no_position)\n",
    "\n",
    "best_no_size_features = all_combinations[best_no_size_features_idx]\n",
    "best_no_color_features = all_combinations[best_no_color_features_idx]\n",
    "best_no_position_features = all_combinations[best_no_position_features_idx]\n",
    "\n",
    "\n",
    "# Combine the best configurations for each variation into a single DataFrame\n",
    "# no_size = ['color', 'position']\n",
    "# no_color = ['size', 'position']\n",
    "# no_position = ['size', 'color']\n",
    "best_configurations_df = pd.DataFrame({\n",
    "    'Feature': ['Size', 'Color', 'Position'],\n",
    "    'Best Completion Time': [min_completion_time_no_size, min_completion_time_no_color, min_completion_time_no_position],\n",
    "    'Size': [-1, best_no_color_features[0], best_no_position_features[1]],\n",
    "    'Color': [best_no_size_features[0], -1, best_no_position_features[1]],\n",
    "    'Position': [best_no_size_features[0], best_no_color_features[1], -1]\n",
    "})\n",
    "\n",
    "best_configurations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config number instead of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with config number instead of size, color, and position\n",
    "X = df_cleaned[['config_num']]\n",
    "y = df_cleaned['completion_time']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "rf_config_num = RandomForestRegressor(random_state=SEED)\n",
    "rf_config_num.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = rf_config_num.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "mse_test = mean_squared_error(y_test, test_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "evaluation_metrics_config_num = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R^2'],\n",
    "    'Value': [mae_test, mse_test, r2_test]\n",
    "})\n",
    "\n",
    "evaluation_metrics_config_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for config_num\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train a random forest model on the config_num feature with the best hyperparameters\n",
    "tuned_rf_config_num = RandomForestRegressor(**best_params)\n",
    "tuned_rf_config_num.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = tuned_rf_config_num.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "mse_test = mean_squared_error(y_test, test_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "tuned_evaluation_metrics_config_num = pd.DataFrame({\n",
    "    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R^2'],\n",
    "    'Value': [mae_test, mse_test, r2_test]\n",
    "})\n",
    "\n",
    "tuned_evaluation_metrics_config_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lowest possible completion time and its configuration using the config_num feature\n",
    "# Find the lowest completion time by going through all possible config numbers\n",
    "# Create a list of all possible config numbers\n",
    "config_nums = np.arange(0, 7)\n",
    "\n",
    "# Predict completion times for all possible config numbers\n",
    "completion_times_config_num = rf_config_num.predict(config_nums.reshape(-1, 1))\n",
    "completion_times_config_num_tuned = tuned_rf_config_num.predict(config_nums.reshape(-1, 1))\n",
    "\n",
    "# Find the lowest completion time\n",
    "min_completion_time_config_num = min(completion_times_config_num)\n",
    "best_config_num = config_nums[np.argmin(completion_times_config_num)]\n",
    "\n",
    "# Find the lowest completion time after tuning\n",
    "min_completion_time_config_num_tuned = min(completion_times_config_num_tuned)\n",
    "best_config_num_tuned = config_nums[np.argmin(completion_times_config_num_tuned)]\n",
    "\n",
    "# Find the highest completion time\n",
    "max_completion_time_config_num = max(completion_times_config_num)\n",
    "worst_config_num = config_nums[np.argmax(completion_times_config_num)]\n",
    "\n",
    "# Find the highest completion time after tuning\n",
    "max_completion_time_config_num_tuned = max(completion_times_config_num_tuned)\n",
    "worst_config_num_tuned = config_nums[np.argmax(completion_times_config_num_tuned)]\n",
    "\n",
    "# Create a dataframe to store the completion times for all possible config numbers including tuned\n",
    "completion_times_config_num_df = pd.DataFrame({\n",
    "    'Config Num': config_nums,\n",
    "    'Completion Time': completion_times_config_num\n",
    "})\n",
    "\n",
    "completion_times_config_num_tuned_df = pd.DataFrame({\n",
    "    'Config Num': config_nums,\n",
    "    'Completion Time': completion_times_config_num_tuned\n",
    "})\n",
    "\n",
    "# Create a DataFrame to store the best and worst completion times and their corresponding configurations\n",
    "configurations_df = pd.DataFrame({\n",
    "    'Configuration': ['Best', 'Worst'],\n",
    "    'Config Num': [best_config_num, worst_config_num],\n",
    "    'Completion Time': [min_completion_time_config_num, max_completion_time_config_num]\n",
    "})\n",
    "\n",
    "configurations_tuned_df = pd.DataFrame({\n",
    "    'Configuration': ['Best', 'Worst'],\n",
    "    'Config Num': [best_config_num_tuned, worst_config_num_tuned],\n",
    "    'Completion Time': [min_completion_time_config_num_tuned, max_completion_time_config_num_tuned]\n",
    "})\n",
    "\n",
    "# Print the best and worst completion times and their corresponding configurations\n",
    "print(\"Best Configuration:\")\n",
    "print(configurations_df[configurations_df['Completion Time'] == min_completion_time_config_num])\n",
    "\n",
    "print(\"\\nWorst Configuration:\")\n",
    "print(configurations_df[configurations_df['Completion Time'] == max_completion_time_config_num])\n",
    "\n",
    "print(\"\\nBest Configuration After Tuning:\")\n",
    "print(configurations_tuned_df[configurations_tuned_df['Completion Time'] == min_completion_time_config_num_tuned])\n",
    "\n",
    "print(\"\\nWorst Configuration After Tuning:\")\n",
    "print(configurations_tuned_df[configurations_tuned_df['Completion Time'] == max_completion_time_config_num_tuned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
